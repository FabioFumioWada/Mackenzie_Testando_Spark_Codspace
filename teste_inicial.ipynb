{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36728a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/02 16:06:13 WARN Utils: Your hostname, codespaces-844813, resolves to a loopback address: 127.0.0.1; using 10.0.10.206 instead (on interface eth0)\n",
      "25/11/02 16:06:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/02 16:06:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession inicializada com sucesso!\n",
      "Versão do Spark: 4.0.1\n",
      "Aplicação: AulaPraticaPySpark_DataQuality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45196)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45198)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45206)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45214)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39772)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39782)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39786)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39802)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 297, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 292, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Teste Fumio\"\n",
    "#Validando commit, push, branch e codspace\"\n",
    "#Este comentário está sendo inscrito no VSCODE do Codspace.\n",
    "    #Instalei o Pyspark no Codspace.\n",
    "#A sincronização entre CODSPACE e VSCODE foi realizada com sucesso\n",
    "    #Criando novos comentários para documentação\n",
    "\n",
    "# Importa o Spark e outras bibliotecas necessárias\n",
    "import pyspark # O módulo principal\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F # O alias 'F' MAIÚSCULO é usado para todas as funções (F.col, F.when, etc.)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "# Cria ou obtém uma SparkSession\n",
    "# Master: 'local[*]' indica que o Spark deve usar todos os núcleos disponíveis na máquina\n",
    "# appName: Nome da aplicação (importante para monitoramento)\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"AulaPraticaPySpark_DataQuality\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") # Opcional: Configurações de memória\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Opcional: Configurações de memória\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Imprime o status da sessão para confirmação\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"Aplicação: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3019020",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_vendas = StructType([\n",
    "    StructField(\"ID_VENDA\", IntegerType(), nullable=False),\n",
    "    StructField(\"DATA_REGISTRO_RAW\", StringType(), nullable=True), # O tipo original é String, pois o formato é inconsistente ('2024-03-20' vs '2024/03/20')\n",
    "    StructField(\"VALOR_BRUTO_RAW\", StringType(), nullable=True),  # O tipo original é String, pois pode vir com vírgula (250,99) ou nulo\n",
    "    StructField(\"PRODUTO\", StringType(), nullable=True),\n",
    "    StructField(\"STATUS_VENDA\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c4acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_arquivo = \"vendas_brutas.csv\" \n",
    "df_vendas_raw = (\n",
    "    spark.read\n",
    "    .csv(\n",
    "        caminho_arquivo,\n",
    "        header=True,\n",
    "        schema=schema_vendas,\n",
    "        sep=\",\",\n",
    "        # O PySpark lê a primeira linha como cabeçalho\n",
    "        # e garante que os dados sigam o schema definido (Data Quality!)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628bcedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Schema Original (Raw) ---\n",
      "root\n",
      " |-- ID_VENDA: integer (nullable = true)\n",
      " |-- DATA_REGISTRO_RAW: string (nullable = true)\n",
      " |-- VALOR_BRUTO_RAW: string (nullable = true)\n",
      " |-- PRODUTO: string (nullable = true)\n",
      " |-- STATUS_VENDA: string (nullable = true)\n",
      "\n",
      "\n",
      "--- Primeiros Registros ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 16:49:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID_VENDA, DATA_REGISTRO, VALOR_BRUTO, PRODUTO, STATUS\n",
      " Schema: ID_VENDA, DATA_REGISTRO_RAW, VALOR_BRUTO_RAW, PRODUTO, STATUS_VENDA\n",
      "Expected: DATA_REGISTRO_RAW but found: DATA_REGISTRO\n",
      "CSV file: file:///workspaces/Mackenzie_Testando_Spark_Codspace/vendas_brutas.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+-------+------------+\n",
      "|ID_VENDA|DATA_REGISTRO_RAW|VALOR_BRUTO_RAW|PRODUTO|STATUS_VENDA|\n",
      "+--------+-----------------+---------------+-------+------------+\n",
      "|1001    |2024-01-15       |150.50         |Laptop |Concluido   |\n",
      "|1002    |2024-02-10       |99.00          |Mouse  |NULL        |\n",
      "|1003    |2024/03/20       |250            |99     |Teclado     |\n",
      "|1004    |2024-04-01       |NULL           |Monitor|Concluido   |\n",
      "|1004    |2024-04-01       |NULL           |Monitor|Concluido   |\n",
      "+--------+-----------------+---------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Schema Original (Raw) ---\")\n",
    "df_vendas_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Primeiros Registros ---\")\n",
    "df_vendas_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fff589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Limpeza (Cleaning): Nulos Tratados ---\n",
      "+--------+-----------------+---------------+-------+----------------+\n",
      "|ID_VENDA|DATA_REGISTRO_RAW|VALOR_BRUTO_RAW|PRODUTO|STATUS_VENDA    |\n",
      "+--------+-----------------+---------------+-------+----------------+\n",
      "|1001    |2024-01-15       |150.50         |Laptop |Concluido       |\n",
      "|1002    |2024-02-10       |99.00          |Mouse  |EM_PROCESSAMENTO|\n",
      "|1003    |2024/03/20       |250            |99     |Teclado         |\n",
      "|1004    |2024-04-01       |0.0            |Monitor|Concluido       |\n",
      "|1004    |2024-04-01       |0.0            |Monitor|Concluido       |\n",
      "+--------+-----------------+---------------+-------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 16:50:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID_VENDA, DATA_REGISTRO, VALOR_BRUTO, PRODUTO, STATUS\n",
      " Schema: ID_VENDA, DATA_REGISTRO_RAW, VALOR_BRUTO_RAW, PRODUTO, STATUS_VENDA\n",
      "Expected: DATA_REGISTRO_RAW but found: DATA_REGISTRO\n",
      "CSV file: file:///workspaces/Mackenzie_Testando_Spark_Codspace/vendas_brutas.csv\n"
     ]
    }
   ],
   "source": [
    "# Ação 1: Limpar/Preencher o campo STATUS_VENDA\n",
    "# Se o status for nulo, vamos preencher com 'EM_PROCESSAMENTO' (regra de negócio)\n",
    "df_limpeza = df_vendas_raw.withColumn(\n",
    "    \"STATUS_VENDA\",\n",
    "    F.coalesce(F.col(\"STATUS_VENDA\"), F.lit(\"EM_PROCESSAMENTO\")) # Coalesce pega o primeiro valor não nulo\n",
    ")\n",
    "\n",
    "# Ação 2: Tratamento de nulos em VALOR_BRUTO_RAW\n",
    "# Se o valor for nulo, vamos preencher com 0 (regra de negócio para vendas não registradas)\n",
    "df_limpeza = df_limpeza.na.fill(value=\"0.0\", subset=['VALOR_BRUTO_RAW'])\n",
    "\n",
    "print(\"\\n--- 1. Limpeza (Cleaning): Nulos Tratados ---\")\n",
    "df_limpeza.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143ff61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Transformação: Padronização e Casting ---\n",
      "root\n",
      " |-- ID_VENDA: integer (nullable = true)\n",
      " |-- PRODUTO: string (nullable = true)\n",
      " |-- STATUS_VENDA: string (nullable = false)\n",
      " |-- VALOR_VENDA: double (nullable = true)\n",
      " |-- DATA_VENDA: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[ID_VENDA: int, PRODUTO: string, STATUS_VENDA: string, VALOR_VENDA: double, DATA_VENDA: date]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformacao = (\n",
    "    df_limpeza\n",
    "    # Transformação 1: Padronizar STATUS_VENDA para caixa alta (CONSISTÊNCIA)\n",
    "    .withColumn(\"STATUS_VENDA\", F.upper(F.col(\"STATUS_VENDA\")))\n",
    "\n",
    "    # Transformação 2: Limpar e Converter VALOR_BRUTO_RAW para Decimal (TIPAGEM CORRETA)\n",
    "    # 1. Substitui vírgulas (',') por ponto ('.')\n",
    "    # 2. Converte o resultado para tipo Double\n",
    "    .withColumn(\n",
    "        \"VALOR_VENDA\",\n",
    "        F.regexp_replace(F.col(\"VALOR_BRUTO_RAW\"), \",\", \".\").cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    # Transformação 3: Converter DATA_REGISTRO_RAW para Tipo Date (FORMATO CORRETO)\n",
    "    # Tenta inferir o formato de data (yyyy-MM-dd, yyyy/MM/dd, etc.) - PySpark é flexível\n",
    "    .withColumn(\n",
    "        \"DATA_VENDA\",\n",
    "        F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy-MM-dd\") # Tenta formato padrão, o PySpark pode inferir variações simples\n",
    "    )\n",
    "    .withColumn(\"DATA_VENDA\", F.coalesce(F.col(\"DATA_VENDA\"), F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy/MM/dd\"))) # Tenta o formato com barra, caso o anterior falhe\n",
    "    .drop(\"DATA_REGISTRO_RAW\", \"VALOR_BRUTO_RAW\") # Remove as colunas RAW\n",
    ")\n",
    "\n",
    "print(\"\\n--- 2. Transformação: Padronização e Casting ---\")\n",
    "df_transformacao.printSchema()\n",
    "#df_transformacao.show(truncate=False)\n",
    "df_transformacao.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e8fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Enriquecimento: Nova Feature Adicionada ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[ID_VENDA: int, PRODUTO: string, STATUS_VENDA: string, VALOR_VENDA: double, DATA_VENDA: date, FLAG_ALTO_VALOR: boolean]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ação: Criar uma nova coluna indicando se a venda é considerada de \"Alto Valor\" (> 200)\n",
    "df_enriquecimento = df_transformacao.withColumn(\n",
    "    \"FLAG_ALTO_VALOR\",\n",
    "    F.when(F.col(\"VALOR_VENDA\") >= 200, F.lit(True)).otherwise(F.lit(False))\n",
    ")\n",
    "\n",
    "print(\"\\n--- 3. Enriquecimento: Nova Feature Adicionada ---\")\n",
    "#df_enriquecimento.show(truncate=False)\n",
    "df_enriquecimento.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc886bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Normalização: Categorias Padronizadas ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[ID_VENDA: int, STATUS_VENDA: string, VALOR_VENDA: double, DATA_VENDA: date, FLAG_ALTO_VALOR: boolean, PRODUTO: string]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ação: Padronizar o nome do produto (ex: 'LAPTOP' pode vir como 'NoteBook', 'Laptop')\n",
    "# Vamos usar o PRODUTO para garantir que todos sejam capitalizados\n",
    "df_normalizacao = df_enriquecimento.withColumn(\n",
    "    \"PRODUTO_PADRAO\",\n",
    "    F.when(F.upper(F.col(\"PRODUTO\")).like(\"%LAPTOP%\"), F.lit(\"NOTEBOOK/LAPTOP\"))\n",
    "    .when(F.upper(F.col(\"PRODUTO\")).like(\"%MOUSE%\"), F.lit(\"PERIFERICO_SIMPLES\"))\n",
    "    .otherwise(F.upper(F.col(\"PRODUTO\")))\n",
    ").drop(\"PRODUTO\").withColumnRenamed(\"PRODUTO_PADRAO\", \"PRODUTO\") # Substitui a coluna original\n",
    "\n",
    "print(\"\\n--- 4. Normalização: Categorias Padronizadas ---\")\n",
    "#df_normalizacao.show(truncate=False)\n",
    "df_normalizacao.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17c046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Desduplicação: Registros Únicos (5 vs 4) ---\n",
      "root\n",
      " |-- ID_VENDA: integer (nullable = true)\n",
      " |-- STATUS_VENDA: string (nullable = false)\n",
      " |-- VALOR_VENDA: double (nullable = true)\n",
      " |-- DATA_VENDA: date (nullable = true)\n",
      " |-- FLAG_ALTO_VALOR: boolean (nullable = false)\n",
      " |-- PRODUTO: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usamos apenas as colunas que definem a unicidade do registro (chave primária natural)\n",
    "colunas_chave = [\"ID_VENDA\"]\n",
    "\n",
    "# Ação: Remover duplicatas estritas\n",
    "df_curated = df_normalizacao.dropDuplicates(subset=colunas_chave)\n",
    "\n",
    "# Demonstração de Desduplicação Avançada (Teoria: Manter o Mais Recente)\n",
    "# Isso é útil quando ID_VENDA pudesse se repetir com versões diferentes.\n",
    "# w = Window.partitionBy(\"ID_VENDA\").orderBy(F.col(\"DATA_VENDA\").desc())\n",
    "# df_curated_avancado = (df_normalizacao\n",
    "#     .withColumn(\"row_number\", F.row_number().over(w))\n",
    "#     .filter(F.col(\"row_number\") == 1)\n",
    "#     .drop(\"row_number\")\n",
    "# )\n",
    "\n",
    "print(f\"\\n--- 5. Desduplicação: Registros Únicos ({df_normalizacao.count()} vs {df_curated.count()}) ---\")\n",
    "#df_curated.show(truncate=False)\n",
    "df_curated.show\n",
    "df_curated.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39068477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 16:55:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID_VENDA, DATA_REGISTRO, VALOR_BRUTO, PRODUTO, STATUS\n",
      " Schema: ID_VENDA, DATA_REGISTRO_RAW, VALOR_BRUTO_RAW, PRODUTO, STATUS_VENDA\n",
      "Expected: DATA_REGISTRO_RAW but found: DATA_REGISTRO\n",
      "CSV file: file:///workspaces/Mackenzie_Testando_Spark_Codspace/vendas_brutas.csv\n",
      "25/11/02 16:55:34 ERROR Executor: Exception in task 0.0 in stage 25.0 (TID 17)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/11/02 16:55:34 WARN TaskSetManager: Lost task 0.0 in stage 25.0 (TID 17) (69f45174-eaeb-4921-8cc4-a69dfd8d101f.internal.cloudapp.net executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/11/02 16:55:34 ERROR TaskSetManager: Task 0 in stage 25.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateTimeException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m caminho_parquet = \u001b[33m\"\u001b[39m\u001b[33mdata/curated/vendas_parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ação: Salvar o DataFrame tratado (df_curated) em Parquet\u001b[39;00m\n\u001b[32m      5\u001b[39m (\n\u001b[32m      6\u001b[39m     \u001b[43mdf_curated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Sobrescreve se o diretório já existir\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSTATUS_VENDA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Particionamento físico: otimiza queries que filtram por status\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msnappy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Snappy é o padrão (bom trade-off)\u001b[39;49;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_parquet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 1. Escrita em Parquet concluída ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDados salvos e particionados em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaminho_parquet\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mDateTimeException\u001b[39m: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "# Definindo o caminho de saída (simulação de um Data Lake)\n",
    "caminho_parquet = \"data/curated/vendas_parquet\"\n",
    "\n",
    "# Ação: Salvar o DataFrame tratado (df_curated) em Parquet\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\") # Sobrescreve se o diretório já existir\n",
    "    .partitionBy(\"STATUS_VENDA\") # Particionamento físico: otimiza queries que filtram por status\n",
    "    .option(\"compression\", \"snappy\") # Snappy é o padrão (bom trade-off)\n",
    "    .parquet(caminho_parquet)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 1. Escrita em Parquet concluída ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442e0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 16:58:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID_VENDA, DATA_REGISTRO, VALOR_BRUTO, PRODUTO, STATUS\n",
      " Schema: ID_VENDA, DATA_REGISTRO_RAW, VALOR_BRUTO_RAW, PRODUTO, STATUS_VENDA\n",
      "Expected: DATA_REGISTRO_RAW but found: DATA_REGISTRO\n",
      "CSV file: file:///workspaces/Mackenzie_Testando_Spark_Codspace/vendas_brutas.csv\n",
      "25/11/02 16:58:55 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 18)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/11/02 16:58:55 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 18) (69f45174-eaeb-4921-8cc4-a69dfd8d101f.internal.cloudapp.net executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/11/02 16:58:55 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateTimeException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m caminho_orc = \u001b[33m\"\u001b[39m\u001b[33mdata/curated/vendas_orc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ação: Salvar o mesmo DataFrame em ORC\u001b[39;00m\n\u001b[32m      5\u001b[39m (\n\u001b[32m      6\u001b[39m     \u001b[43mdf_curated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSTATUS_VENDA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43morc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_orc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 2. Escrita em ORC concluída ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDados salvos e particionados em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaminho_orc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2288\u001b[39m, in \u001b[36mDataFrameWriter.orc\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2286\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2287\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2288\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43morc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mDateTimeException\u001b[39m: [CANNOT_PARSE_TIMESTAMP] Text '2024/03/20' could not be parsed at index 4. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "# Definindo o caminho de saída\n",
    "caminho_orc = \"data/curated/vendas_orc\"\n",
    "\n",
    "# Ação: Salvar o mesmo DataFrame em ORC\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"STATUS_VENDA\")\n",
    "    .orc(caminho_orc)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 2. Escrita em ORC concluída ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_orc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203ddd2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually. SQLSTATE: 42KD9",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Leitura Otimizada - Filtro na partição\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Apenas os diretórios (partições) que contêm 'CANCELADO' serão lidos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_leitura_filtrada = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_parquet\u001b[49m\u001b[43m)\u001b[49m.filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mSTATUS_VENDA\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mCANCELADO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 3.1. Predicate Pushdown: Filtro no Disco ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal de Registros lidos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_leitura_filtrada.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually. SQLSTATE: 42KD9"
     ]
    }
   ],
   "source": [
    "# Leitura Otimizada - Filtro na partição\n",
    "# Apenas os diretórios (partições) que contêm 'CANCELADO' serão lidos\n",
    "df_leitura_filtrada = spark.read.parquet(caminho_parquet).filter(F.col(\"STATUS_VENDA\") == \"CANCELADO\")\n",
    "\n",
    "print(\"\\n--- 3.1. Predicate Pushdown: Filtro no Disco ---\")\n",
    "print(f\"Total de Registros lidos: {df_leitura_filtrada.count()}\")\n",
    "df_leitura_filtrada.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef775a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually. SQLSTATE: 42KD9",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Leitura Otimizada - Seleção de Colunas (Projeção)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# O Spark carrega apenas os bytes da coluna 'ID_VENDA' e 'VALOR_VENDA'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_projecao = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_parquet\u001b[49m\u001b[43m)\u001b[49m.select(\u001b[33m\"\u001b[39m\u001b[33mID_VENDA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVALOR_VENDA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 3.2. SerDe: Deserialização Seletiva (Projeção) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColunas carregadas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_projecao.columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually. SQLSTATE: 42KD9"
     ]
    }
   ],
   "source": [
    "# Leitura Otimizada - Seleção de Colunas (Projeção)\n",
    "# O Spark carrega apenas os bytes da coluna 'ID_VENDA' e 'VALOR_VENDA'\n",
    "df_projecao = spark.read.parquet(caminho_parquet).select(\"ID_VENDA\", \"VALOR_VENDA\")\n",
    "\n",
    "print(\"\\n--- 3.2. SerDe: Deserialização Seletiva (Projeção) ---\")\n",
    "print(f\"Colunas carregadas: {df_projecao.columns}\")\n",
    "df_projecao.printSchema()\n",
    "\n",
    "# Ponto de Discussão:\n",
    "# Comparação 1: Se este fosse um arquivo CSV, o SerDe leria a linha inteira para depois descartar as colunas não pedidas.\n",
    "# Comparação 2: No Parquet, o Spark/SerDe acessa apenas a 'faixa' vertical da coluna no arquivo físico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0bab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
